---
title: 'Enter Proxmox and ZFS'
date: 2020-05-07
author: Jan
layout: single
categories:
  - Linux / Unix
tags:
  - zfs
  - proxmox
published: false
---
Following up on my [Exit Synology](/2016/07/15/my-history-in-gadgets-update-2016/) post, I've decided it's time to move
from a consumer grade NAS to something a bit more sturdy. I've also been running out of memory on the Synology NAS with 
all the things that I wanted to run on it... so time for something else.

Functionalities I wanted to replace:
* [NAS](https://en.wikipedia.org/wiki/Network-attached_storage)
* [Hypervisor](https://en.wikipedia.org/wiki/Hypervisor)
* [Container](https://en.wikipedia.org/wiki/OS-level_virtualization) runtime
* VPN endpoint
* file sync (like dropbox)
* Cloud backup
* Backup target for other devices 

I went through a few iterations before deciding on my final setup:

## First iteration: FreeNAS + bhyve hypervisor

Quoting the [FreeNAS](https://freenas.org) website: 
> FreeNAS is an operating system that can be installed on virtually any hardware platform to share data over a network.
> FreeNAS is the simplest way to create a centralized and easily accessible place for your data. 
> Use FreeNAS with ZFS to protect, store, and back up all of your data. FreeNAS is used everywhere, for the home, 
> small business, and the enterprise.

This seemed like a good solution for the NAS part of the equation. It's built on [FreeBSD](https://freebsd.org), 
offers [ZFS](https://www.freebsd.org/doc/handbook/zfs.html) for storage, a bunch of [plugins](https://www.freenas.org/plugins/)
to extend the functionalities of FreeNAS beyond what the barebones offers... a bit like Synology does.

For the hypervisor part, it's less ideal.  
[bhyve](https://bhyve.org/), the FreeBSD hypervisor, isn't quite as optimized yet as many other hypervisors out there,
and [tests](https://www.ixsystems.com/community/threads/virtual-machine-performance-11-2-u3.76224/) 
[show](https://www.reddit.com/r/homelab/comments/9vz26t/hypervisor_performance_comparison/) that there is
quite a performance penalty still - this is getting [a lot better](https://b3n.org/vmware-vs-bhyve-performance-comparison/)
in FreeBSD 12, but FreeNAS is still based on 11 (right now)

FreeNAS also has no solution for running containers except for running them in a VM. While this isn't an issue perse, it
does require you to actually run another layer of virtualisation in between.

### Second iteration: VMWare ESXi + FreeNAS VM
So, to solve the virtualisation issue, there was the halfbaked idea of running [VMWare](https://vmware.com)'s 
[ESXi](https://www.vmware.com/be/products/esxi-and-esx.html) as a hypervisor, run FreeNAS as a VM and share the storage
back to ESXi for running VM's on.

This also introduces somewhat a chicken-and-egg issue - FreeNAS needs to be started before the other VM's, and other 
VM's need to be shutdown before stopping FreeNAS. *Chaos, panic and disorder - my work is done!*

Problems:
* ESXi is a proprietary product by VMWare. The free version comes with no support, and you have no guarantee that they 
won't discontinue some hardware support. 
* ESXi needs to run on something. A USB thumbdrive works, but if you want some reliability it's advisable to use some
form of hardware RAID. ESXi doesn't do software RAID.
* For FreeNAS [PCI passthrough](https://en.wikipedia.org/wiki/X86_virtualization#I/O_MMU_virtualization_(AMD-Vi_and_Intel_VT-d))
of the storage controller is required - which means I do need to have two storage controllers - one for ESXi, one for FreeNAS.
* Using NFS (or iSCSI) for ESXi datastores means that you need to run those shared filesystems in 
[sync](https://www.ixsystems.com/community/threads/sync-writes-or-why-is-my-esxi-nfs-so-slow-and-why-is-iscsi-faster.12506/) mode.  
Sync mode is slow (it needs to actively sync every change to disk), so you need an additional [SLOG](https://www.ixsystems.com/community/threads/some-insights-into-slog-zil-with-zfs-on-freenas.13633/) device.  
The SLOG device also needs to have [power loss protection](https://www.ixsystems.com/community/threads/list-of-ssds-with-power-loss-protection.63998/), because you *need* to be sure the writes are consistent.
* The RAM allocated to FreeNAS is 'lost' to the rest of the system.
* ESXi doesn't do containers. Running a VM with a container daemon is the only solution.

### Third iteration: Proxmox and native ZFS

From the [Proxmox](https://proxmox.com/en/proxmox-ve) website:
> Proxmox VE is a complete open-source platform for all-inclusive enterprise virtualization that tightly integrates 
> KVM hypervisor and LXC containers, software-defined storage and networking functionality on a single platform, 
> and easily manages high availability clusters and disaster recovery tools with the built-in web management interface.

This is basically [Debian](https://debian.org), with a hypervisor GUI, running on top of ZFS. It doesn't offer 
anything of the other things, but it being Debian, it's not that hard to make it do all the things that I want it to do.

* NAS functionality will be done with ZFS and sharing through [Samba](https://samba.org) and [NFS](https://en.wikipedia.org/wiki/Network_File_System).  
* Hypervisor - Proxmox covers that base: it supports both lightweight [Linux Containers (LXC)](https://en.wikipedia.org/wiki/LXC),
or full fledged VM's using [KVM](https://www.linux-kvm.org/page/Main_Page).  
* For containers there's the [docker](https://en.wikipedia.org/wiki/Docker_(software)) daemon.

### Final configuration
In the end I settled with Proxmox on top of ZFS, running on a [raid-z1](https://en.wikipedia.org/wiki/Non-standard_RAID_levels#RAID-Z)
configuration with my WD Red drives.

#### Hardware
The hardware for the build is:
* [Sharkoon SilentStorm SFX 500 Gold](https://en.sharkoon.com/product/1681/SilStorSFXGold) PSU
* [Fractal Node 804](https://www.fractal-design.com/products/cases/node/node-804/black/) case
* [Intel Xeon 1230v5](https://ark.intel.com/content/www/us/en/ark/products/88182/intel-xeon-processor-e3-1230-v5-8m-cache-3-40-ghz.html) CPU
* [Cooler Master Hyper 212 EVO](https://www.coolermaster.com/catalog/coolers/cpu-air-coolers/hyper-212-evo/) CPU cooler
* 2 bars of [Kingston 16GB ECC DDR4](https://www.amazon.nl/dp/B071S24N83/ref=pe_19967891_406998361_TE_SCE_dp_1) RAM
* [Asrockrack C236 WS](https://www.asrockrack.com/general/productdetail.asp?Model=C236%20WS#Specifications) mainboard
* 4 [WD Red](https://shop.westerndigital.com/en-ie/products/internal-drives/wd-red-sata-hdd) 6TB [PMR](https://en.wikipedia.org/wiki/Perpendicular_recording) drives

#### Software
* Proxmox (both LXC and VM's)
* Docker CE runtime
* Samba
* NFS

##### Proxmox
Proxmox itself is a fairly standard installation. The changes made were:
* Reduce the ZFS spa_asize_inflation parameter from the default 24 to 6. https://github.com/openzfs/zfs/issues/10373
  ```shell script
  echo 6 > /sys/module/zfs/parameters/spa_asize_inflation
  ```
* Activate the powersave CPU scaling governor
  ```shell script
  echo "powersave" | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
  ```

* defining new [storage endpoints](https://pve.proxmox.com/wiki/Storage) in `/etc/pve/storage.cfg` for backups and images
* configuring the main bridge `vmbr0` to be vlan aware, and defining a new `vmbr0.13` interface to get the specific VLAN
on the machine running Proxmox.
  ```
  auto vmbr0 vmbr0.13

  iface vmbr0.13 inet static
        address 192.168.0.2
        netmask 255.255.255.0
        gateway 192.168.0.1

  iface vmbr0 inet manual
        bridge_vlan_aware yes
        bridge_stp off
        bridge_fd 0
        bridge_ports enp0s31f6

  iface enp0s31f6 inet manual
  ```

##### Linux Containers
I'm running a few Linux Containers (LXC), which are hosting
* [Unifi Network Management Controller](https://www.ui.com/software/) (on a different VLAN)
* General purpose container (for some tools)

##### Virtual Machines
At the moment only one VM is running, which hosts my [WireGuard](https://wireguard.com) endpoint. Reason why this runs
in a VM is because it requires kernel modules, and I don't want to add anything extra to my main Proxmox installation.

#### Docker specific configuration
Docker has been configured to use the [zfs storage driver](https://docs.docker.com/storage/storagedriver/zfs-driver/),
which makes better use of the capabilities of ZFS. Configuring this is as easy as adding
```json5
{
    "storage-driver": "zfs"
}
```
to `/etc/docker/daemon.json`.

For networking I'm using both a user-defined [bridge](https://docs.docker.com/network/bridge/), which has several
advantages over using the default bridge (automatic DNS resolution between containers for one), and
[macvlan](https://docs.docker.com/network/macvlan/) to assign external (from Docker's point of view) IP's to docker 
containers.
 
The macvlan setup is simple:
```shell script
docker network create \ 
  --driver macvlan \ 
  --gateway 192.168.0.1 \  
  --subnet 192.168.0.0/24 \ 
  --ip-range 192.168.0.128/25 \ 
  --opt parent=vmbr0.13 \ 
  macvlan-lan
```

For the actual containers, I'm running a bunch of them:
* [Watchtower](https://containrrr.github.io/watchtower/) to keep containers automatically up-to-date ([dockerhub](https://hub.docker.com/r/v2tec/watchtower))
* [Nginx](https://github.com/nginx-proxy/nginx-proxy) as a reverse proxy in front of the containers - automatically supplying an internal hostname.
  This also removes the requirement to expose the container ports on the host itself ([dockerhub](https://hub.docker.com/r/jwilder/nginx-proxy/))
* [Heimdall](https://heimdall.site/) as an application dashboard ([dockerhub](https://hub.docker.com/r/linuxserver/heimdall))
* [Nextcloud](https://nextcloud.org) for internal file sharing and collaboration ([dockerhub](https://hub.docker.com/_/nextcloud/))
* [MariaDB](https://mariadb.org) for Nextcloud ([dockerhub](https://hub.docker.com/_/mariadb))
* [BackupPC](https://backuppc.github.io/backuppc) for agentless backups ([dockerhub](https://hub.docker.com/r/adferrand/backuppc/))
* [Plex](https://www.plex.tv) for streaming to devices ([dockerhub](https://hub.docker.com/r/plexinc/pms-docker))
* [Portainer](https://portainer.org) for easy Docker Container management ([dockerhub](https://hub.docker.com/r/portainer/portainer/))
* [Grafana](https://grafana.com) for fancy dashboards and graphs ([dockerhub](https://hub.docker.com/r/grafana/grafana))
* [InfluxDB](https://influxdata.com) as a datastore for Grafana (for data coming from Home Assistant) ([dockerhub](https://hub.docker.com/_/influxdb))
* [Duplicati](https://duplicati.com) for compressed de-duplicated and encrypted backup to Backblaze B2 ([dockerhub](https://hub.docker.com/r/linuxserver/duplicati/))
* [PiHole](https://pi-hole.net) for network-wide ad blocking ([dockerhub](https://hub.docker.com/r/pihole/pihole/))
  I'm actually running a few - one per VLAN that is used for internet access.

